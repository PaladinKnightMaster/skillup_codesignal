# Evaluating a Model with PyTorch

## Introduction
Hello! In today's lesson, we will be diving into evaluating models in **PyTorch**. Evaluating the performance of a model plays a key role in the process of building an effective machine learning model. It helps us to understand the ability of the model to generalize on unseen data. We will attain this by using a test dataset, making predictions using our trained model, and comparing these predictions with the actual truth values in the test dataset.

## Recap: Training the Model
Before evaluating the model, let's quickly recap the training process:

```Python
import torch
import torch.nn as nn
import torch.optim as optim

# Training Features
X_train = torch.tensor([
    [3.0, 0.5], [1.0, 1.0], [0.5, 2.0], [2.0, 1.5],
    [3.5, 3.0], [2.0, 2.5], [1.5, 1.0], [0.5, 0.5],
    [2.5, 0.8], [2.1, 2.0], [1.2, 0.5], [0.7, 1.5]
], dtype=torch.float32)

# Training Targets
y_train = torch.tensor([[1], [0], [0], [1], [1], [0], [1], [0], [1], [0], [1], [0]], dtype=torch.float32)

# Define the model using nn.Sequential
model = nn.Sequential(
    nn.Linear(2, 10),
    nn.ReLU(),
    nn.Linear(10, 1),
    nn.Sigmoid()
)

# Define loss function and optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# Train the model for 50 epochs
for epoch in range(50):
    model.train()  # Set the model to training mode
    optimizer.zero_grad()  # Zero the gradients
    outputs = model(X_train)  # Compute predictions
    loss = criterion(outputs, y_train)  # Compute the loss
    loss.backward()  # Compute the gradient
    optimizer.step()  # Update the parameters
```

Hereâ€™s a summary of the steps:

1. **Data Preparation**: Defined training features `X_train` and targets `y_train`.
2. **Model Architecture**: Created a network using `nn.Sequential` with one hidden layer (ReLU) and an output layer (Sigmoid).
3. **Loss and Optimizer**: Used Binary Cross-Entropy Loss (`BCELoss`) and the Adam optimizer.
4. **Training Loop**: Trained for 50 epochs, performing forward pass, loss calculation, backpropagation, and parameter updates in each epoch.

Now let's move to evaluating our model.

## Loading and Preparing the Test Dataset
Before we evaluate our model, we need to prepare our test dataset. The test dataset consists of new data points that the model has never seen before. This helps us understand how well our model generalizes to unseen data.

Let's define our testing data with the same format our model was trained on in the previous lessons:

```Python
import torch

# Test Features
X_test = torch.tensor([[2.5, 1.0], [0.8, 0.8], [1.0, 2.0], [3.0, 2.5]], dtype=torch.float32)
# Test Targets
y_test = torch.tensor([[1], [0], [0], [1]], dtype=torch.float32)
```

With our test data ready, we can now move on to evaluating our model's performance using these new examples.

## Evaluation Metrics

Next, we introduce the concept of **evaluation metrics**. Evaluation metrics interpret the performance of our model. There are many metrics, but we will consider **accuracy** for our lesson.

**Accuracy** is a useful measure when the target variable classes in the data are nearly balanced. It is defined as the number of correct predictions made divided by the total number of predictions made. Mathematically, it is represented as:

\[
\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}}
\]

We can easily apply this using **Scikit-Learn**. The `sklearn.metrics` module includes score functions, performance metrics, pairwise metrics, and distance computations, and we import the function `accuracy_score` from this module.

```Python
# Import the accuracy_score functionality
from sklearn.metrics import accuracy_score
```

## Evaluating the Model
Once the model is trained, we switch it to evaluation mode using `model.eval()`. Finally, it's time to evaluate our model. Here's an outline of the steps followed in the code:

* Generate predictions from the model.
* Convert the predicted probabilities (0 to 1) to binary classes (0 and 1) using a threshold.
* Compute the loss by comparing the predictions with actual targets.
* Calculate the accuracy of the model on the test data.
* Print the test loss and accuracy.

Let's run our model evaluation code:

```Python
# Set evaluation mode and disable gradient
model.eval()
with torch.no_grad():
    # Make Predictions
    outputs = model(X_test)
    # Convert to binary classes
    predicted_classes = (outputs > 0.5).int()
    # Calculate the loss on the test data
    test_loss = criterion(outputs, y_test).item()
    # Calculate the accuracy on the test data
    test_accuracy = accuracy_score(y_test.numpy(), predicted_classes.numpy())

# Print the test accuracy and loss
print(f'\nTest accuracy: {test_accuracy}, Test loss: {test_loss}')
```

The output of the code is the accuracy of the model on the test set and the loss on the test set:

```
Test accuracy: 1.0, Test loss: 0.19731628894805908
```

This output implies that our model has perfectly classified all the test examples correctly, showcasing a 100% accuracy. The very low test loss indicates that the model's predictions are very close to the actual targets. However, it's important to note that while a perfect accuracy and low loss on the test set are desirable, they don't always guarantee that the model will perform equally well on entirely new data. Overfitting can sometimes cause high performance on the test set at the expense of generalizability. Therefore, it's essential to validate these results with a larger and more varied dataset in real-world applications.

## Lesson Summary and Practice
Excellent! You now understand how to evaluate a model in PyTorch. Understanding model performance is core to building effective machine learning models. Going forward, these skills will be crucial for understanding and improving the performance of your models.

All that's left is to practice this skill. Up next, you'll find practice exercises designed to reinforce what you've learned today. Keep going - you're doing great!
