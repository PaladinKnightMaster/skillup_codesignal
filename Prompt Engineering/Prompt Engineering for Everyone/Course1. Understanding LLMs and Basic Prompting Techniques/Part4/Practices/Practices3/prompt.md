<!-- __ASK__ -->

__PROMPT__
Generate a Markdown-formatted table that lists the context limits (token ranges) for several well-known large language models (LLMs). Include the following columns: "Model" and "Token Range". Some of the models to include are GPT-3, GPT-3.5, GPT-4, Claude 2, PaLM-2, LLaMA 2, Mistral 7B, and any other well-known models. Don't worry if the exact context limits aren't publicly known; approximate or estimated ranges are fine for this exercise.

The table should look like this:

| Model        | Token Range |
|--------------|-------------|
| GPT-4        | 4k-32k      |
| Claude 2     | 100k        |
| GPT-3.5      | 4k          |
| PaLM-2       | 8k          |
| LLaMA 2      | 4k-8k       |
| Mistral 7B   | 8k          |
| GPT-3        | 2k          |
